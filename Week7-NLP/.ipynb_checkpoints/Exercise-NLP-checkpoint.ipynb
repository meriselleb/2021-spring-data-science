{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise with Natural Language Processing\n",
    "\n",
    "For todays exersice we will be doing two things.  The first is to build the same model with the same data that we did in the lecture, the second will be to build a new model with new data. \n",
    "\n",
    "## PART 1: \n",
    "- 20 Newsgroups Corpus\n",
    "\n",
    "\n",
    "## PART 2:\n",
    "- Republican vs Democrat Tweet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 952 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 2.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.3.17-cp39-cp39-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 4.3 MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=c1779440a12c25b4cf0b523f7cc4f7666a2985894fc7afabd70ed112ea19c778\n",
      "  Stored in directory: /Users/meriselleruotolo/Library/Caches/pip/wheels/13/ae/bb/5e2a232ebaa1d2f38dd5f587e9fc4cf6ccb12758d14dac14d8\n",
      "Successfully built nltk\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2021.3.17 tqdm-4.59.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/meriselleruotolo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/meriselleruotolo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/meriselleruotolo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import pandas for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK is our Natural-Language-Took-Kit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Libraries for helping us with strings\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "# Import our text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Import our classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Import some ML helper function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Import our metrics to evaluate our model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# You may need to download these from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and display data.\n",
    "1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "1. Print the shape\n",
    "1. Inspect / remove nulls and duplicates\n",
    "1. Find class balances, print out how many of each topic_category there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\r\\...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\r\\nSubject: Re:...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  topic  \\\n",
       "0   0  From: lerxst@wam.umd.edu (where's my thing)\\r\\...      7   \n",
       "1   1  From: guykuo@carson.u.washington.edu (Guy Kuo)...      4   \n",
       "2   2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...      4   \n",
       "3   3  From: jgreen@amber (Joe Green)\\r\\nSubject: Re:...      1   \n",
       "4   4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...     14   \n",
       "\n",
       "          topic_category  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "# 2. Print the shape\n",
    "df = pd.read_csv('data/20-newsgroups.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                0\n",
      "message           0\n",
      "topic             0\n",
      "topic_category    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3. Inspect / remove nulls and duplicates\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.sport.hockey            600\n",
       "soc.religion.christian      599\n",
       "rec.motorcycles             598\n",
       "rec.sport.baseball          597\n",
       "sci.crypt                   595\n",
       "sci.med                     594\n",
       "rec.autos                   594\n",
       "sci.space                   593\n",
       "comp.windows.x              593\n",
       "sci.electronics             591\n",
       "comp.os.ms-windows.misc     591\n",
       "comp.sys.ibm.pc.hardware    590\n",
       "misc.forsale                585\n",
       "comp.graphics               584\n",
       "comp.sys.mac.hardware       578\n",
       "talk.politics.mideast       564\n",
       "talk.politics.guns          546\n",
       "alt.atheism                 480\n",
       "talk.politics.misc          465\n",
       "talk.religion.misc          377\n",
       "Name: topic_category, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Find class balances, print out how many of each topic_category there are.\n",
    "df.topic_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing \n",
    "(aka Feature engineering)\n",
    "1. Make a function that makes all text lowercase.\n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "    \n",
    "2. Make a function that removes all punctuation. \n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "    \n",
    "3. Make a function that removes all stopwords.\n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "    \n",
    "4. EXTRA CREDIT (This step only): Make a function that stemms all words. \n",
    "\n",
    "\n",
    "5. Mandatory: Make a pipeline function that applys all the text processing functions you just built.\n",
    "    * Do a sanity check by feeding in a test sentence into the pipeline. \n",
    "    \n",
    "    \n",
    "    \n",
    "6. Mandatory: Use `df['message_clean'] = df[column].apply(???)` and apply the text pipeline to your text data column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sentence with lots of caps.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Make a function that makes all text lowercase.\n",
    "def make_lower(a_string):\n",
    "    return a_string.lower()\n",
    "\n",
    "test_string = 'This is A SENTENCE with LOTS OF CAPS.'\n",
    "make_lower(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sentence 50 With lots of punctuation  other things'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Make a function that removes all punctuation. \n",
    "\n",
    "def remove_punctuation(a_string):\n",
    "    a_string = re.sub(r'[^\\w\\s]','', a_string)\n",
    "    return a_string\n",
    "\n",
    "test_string = 'This is a sentence! 50 With lots of punctuation??? & other #things.'\n",
    "remove_punctuation(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This sentence ! With different stopwords added .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Make a function that removes all stopwords.\n",
    "def remove_stopwords(a_string):\n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "    \n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        \n",
    "        # Check if word is not in stopwords\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            # If word not in stopwords, append to our valid_words\n",
    "            valid_words.append(word)\n",
    "\n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string\n",
    "\n",
    "test_string = 'This is a sentence! With some different stopwords i have added in here.'\n",
    "remove_stopwords(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I play and start play with player and we all love to play with play'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. EXTRA CREDIT: Make a function that stemms all words. \n",
    "def stem_words(a_string):\n",
    "    # Initalize our Stemmer\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "\n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        # Stem the word\n",
    "        stemmed_word = porter.stem(word)\n",
    "        \n",
    "        # Append stemmed word to our valid_words\n",
    "        valid_words.append(stemmed_word)\n",
    "        \n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string \n",
    "\n",
    "test_string = 'I played and started playing with players and we all love to play with plays'\n",
    "stem_words(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'played started playing players love play plays'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. MANDATORY: Make a pipeline function that applys all the text processing functions you just built.\n",
    "def text_pipeline(input_string):\n",
    "    input_string = make_lower(input_string)\n",
    "    input_string = remove_punctuation(input_string)\n",
    "    input_string = remove_stopwords(input_string)    \n",
    "    return input_string\n",
    "\n",
    "\n",
    "test_string = 'I played and started playing with players and we all love to play with plays'\n",
    "text_pipeline(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT: From: lerxst@wam.umd.edu (where's my thing)\r\n",
      "Subject: WHAT car is this!?\r\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\r\n",
      "Organization: University of Maryland, College Park\r\n",
      "Lines: 15\r\n",
      "\r\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\r\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\r\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\r\n",
      "the front bumper was separate from the rest of the body. This is \r\n",
      "all I know. If anyone can tellme a model name, engine specs, years\r\n",
      "of production, where this car is made, history, or whatever info you\r\n",
      "have on this funky looking car, please e-mail.\r\n",
      "\r\n",
      "Thanks,\r\n",
      "- IL\r\n",
      "   ---- brought to you by your neighborhood Lerxst ----\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "CLEANDED TEXT: lerxstwamumdedu wheres thing subject car nntppostinghost rac3wamumdedu organization university maryland college park lines 15 wondering anyone could enlighten car saw day 2door sports car looked late 60s early 70s called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please email thanks il brought neighborhood lerxst\n"
     ]
    }
   ],
   "source": [
    "# 6. Mandatory: Use `df[column].apply(???)` and apply the text pipeline to your text data column. \n",
    "df['message_clean'] = df['message']\n",
    "df['message_clean'] = df['message_clean'].apply(make_lower)\n",
    "df['message_clean'] = df['message_clean'].apply(remove_punctuation)\n",
    "# df['message_clean'] = df['message_clean'].apply(lem_with_pos_tag)\n",
    "df['message_clean'] = df['message_clean'].apply(remove_stopwords)\n",
    "df['message_clean'] = df['message'].apply(text_pipeline)\n",
    "\n",
    "print(\"ORIGINAL TEXT:\", df['message'][0])\n",
    "print(\"CLEANDED TEXT:\", df['message_clean'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "1. Define your `X` and `y` data. \n",
    "\n",
    "\n",
    "2. Initialize a vectorizer (you can use TFIDF or BOW, it is your choice).\n",
    "    * Do you want to use n-grams..?\n",
    "\n",
    "\n",
    "3. Fit your vectorizer using your X data.\n",
    "    * Remember, this process happens IN PLACE.\n",
    "\n",
    "\n",
    "4. Transform your X data using your fitted vectorizer. \n",
    "    * `X = vectorizer.???`\n",
    "\n",
    "\n",
    "\n",
    "5. Print the shape of your X.  How many features (aka columns) do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your `X` and `y` data. \n",
    "X = df['message_clean'].values\n",
    "\n",
    "y = df['topic_category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize a vectorizer (you can use TFIDF or BOW, it is your choice).\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Fit your vectorizer using your X data\n",
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transform your X data using your fitted vectorizer. \n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 139767) <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# 5. Print the shape of your X.  How many features (aka columns) do you have?\n",
    "print(X.shape, type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split your data into Training and Testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into testing and training like always. \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Build and Train Model\n",
    "Use Multinomial Naive Bayes to classify these documents. \n",
    "\n",
    "1. Initalize an empty model. \n",
    "2. Fit the model with our training data.\n",
    "\n",
    "\n",
    "Experiment with different alphas.  Use the alpha gives you the best result.\n",
    "\n",
    "EXTRA CREDIT:  Use grid search to programmatically do this for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initalize an empty model. \n",
    "model = MultinomialNB(alpha = .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.916483\n"
     ]
    }
   ],
   "source": [
    "# Fit our model with our training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Model Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model.\n",
    "\n",
    "1. Make new predicitions using our test data. \n",
    "2. Print the accuracy of the model. \n",
    "3. Print the confusion matrix of our predictions. \n",
    "4. Using `classification_report` print the evaluation results for all the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make new predictions of our testing data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Print the accuracy of the model. \n",
    "accuracy = ???\n",
    "\n",
    "print(\"Model Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Plot the confusion matrix of our predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Using `classification_report` print the evaluation results for all the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual predicition\n",
    "Write a new sentence that you think will be classified as talk.politics.guns. \n",
    "1. Apply the text pipeline to your sentence\n",
    "2. Transform your cleaned text using the `X = vectorizer.transform([your_text])`\n",
    "    * Note, the `transform` function accepts a list and not a individual string.\n",
    "3. Use the model to predict your new `X`. \n",
    "4. Print the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = ???\n",
    "\n",
    "# 1. Apply the text pipeline to your sentence\n",
    "\n",
    "# 2. Transform your cleaned text using the `X = vectorizer.transform([your_text])`\\\n",
    "\n",
    "# 3. Use the model to predict your new `X`. \n",
    "\n",
    "# 4. Print the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# PART 2: Twitter Data\n",
    "This part of the exercise is un-guided on purpose.  \n",
    "\n",
    "Using the `dem-vs-rep-tweets.csv` build a classifier to determine if a tweet was written by a democrat or republican. \n",
    "\n",
    "Can you get an f1-score higher than %82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "# 2. Print the shape\n",
    "df = pd.read_csv('data/dem-vs-rep-tweets.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
